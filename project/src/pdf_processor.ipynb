{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3056243b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks gerados: 60\n",
      "Exemplo de chunk com metadados:\n",
      " {'chunk_number': 0, 'text': 'CONTRATO DE ASSOCIA√á√ÉO DE EMPRESA Pelo presente instrumento e na melhor forma de direito, de um lado, AJJ SERVI√áOS CONTAB√âIS LTDA, pessoa jur√≠dica de direito privado, inscrita no CNPJ sob o n¬∫ 28.913.706/0001-63, com sede na Rua Alfred Charvet, n¬∫ 710, Bairro Vila Nova, Arauc√°ria/PR, CEP: 83.703-278, neste ato, representada por seu s√≥cio administrador, Jhonny C√©zar de Jesus Falavinha, brasileiro, portador do RG n¬∫ 6.223.402-4 SSP/PR, inscrito no CPF n¬∫ 007.724.809-07, doravante denominada EMPRESA ASSOCIADA, e de outro lado, o PARQUE CIENT√çFICO E TECNOL√ìGICO DE BIOCI√äNCIAS LTDA, pessoa jur√≠dica de direito privado, inscrito no CNPJ sob o n¬∫ 21.526.709/0001-03, com sede na Rodovia PR-182, Km 320/321, s/n, Condom√≠nio Industrial Biopark, no munic√≠pio de Toledo, Estado do Paran√°, CEP 85.919-899, neste ato representada por seu s√≥cio administrador, Victor Donaduzzi, brasileiro, inscrito no CPF sob o n¬∫ 047.139.439-40, doravante denominado BIOPARK. As Partes resolvem celebrar o presente Contrato de Associa√ß√£o de Empresa, em observ√¢ncia √† legisla√ß√£o aplic√°vel ao caso e em conformidade com as cl√°usulas e condi√ß√µes a seguir: PRE√ÇMBULO Para os fins deste contrato, considera-se EMPRESA ASSOCIADA aquela formalmente vinculada ao BIOPARK, que utiliza de seus espa√ßos e servi√ßos para desenvolver seus neg√≥cios, por√©m, com presen√ßa di√°ria no territ√≥rio. CL√ÅUSULA PRIMEIRA ‚Äì DO OBJETO O presente contrato tem como objeto a presta√ß√£o de servi√ßos listados a seguir, por parte do BIOPARK: Par√°grafo Primeiro: Constitui objeto deste contrato a presta√ß√£o de', 'source_file': 'AJJ.SERVICOS.CONTABEIS.LTDA.-.Contrato.de.Empresa.Associada.1.pdf', 'page_number': 1}\n",
      "‚úÖ 60 chunks salvos em D:\\Ciencia de Dados\\Projeto Integrador de Exten√ß√£o 3\\RAGzilla\\project\\data\\processed\\chunks.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pdfplumber\n",
    "import re\n",
    "import json\n",
    "import tiktoken\n",
    "import spacy\n",
    "\n",
    "# ==============================================\n",
    "# 1. Configura√ß√£o do ambiente\n",
    "# ==============================================\n",
    "# Carrega vari√°veis de ambiente do arquivo .env (ex: PDF_FOLDER e OUTPUT_FOLDER)\n",
    "load_dotenv()\n",
    "pdf_folder = os.getenv(\"PDF_FOLDER\")\n",
    "output_folder = os.getenv(\"OUTPUT_FOLDER\")\n",
    "\n",
    "# ==============================================\n",
    "# 2. Fun√ß√µes de limpeza e extra√ß√£o de entidades\n",
    "# ==============================================\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Limpa o texto extra√≠do do PDF.\n",
    "    - Substitui quebras de linha por espa√ßo.\n",
    "    - Remove espa√ßos extras consecutivos.\n",
    "    - Remove '__' que podem aparecer como lixo no PDF.\n",
    "    - Corrige quebras no meio de n√∫meros, como '28. 913' -> '28.913'.\n",
    "    - Remove marca√ß√µes de p√°gina, ex: \"P√°gina 1 de 12\".\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    text = re.sub(r'\\n+', ' ', text)  # remove m√∫ltiplas quebras de linha\n",
    "    text = re.sub(r'\\s+', ' ', text)  # normaliza espa√ßos\n",
    "    text = re.sub(r'__', '', text)    # remove '__'\n",
    "    text = re.sub(r'(\\d{2,3})\\.\\s+(\\d{3})', r'\\1.\\2', text)  # corrige n√∫meros quebrados\n",
    "    text = re.sub(r'P√°gina\\s+\\d+\\s+de\\s+\\d+', '', text, flags=re.IGNORECASE)  # remove \"P√°gina X de Y\"\n",
    "    return text.strip()\n",
    "\n",
    "def extract_entities_regex(text):\n",
    "    \"\"\"\n",
    "    Extrai documentos do texto usando express√µes regulares:\n",
    "    - CNPJs: 00.000.000/0000-00\n",
    "    - CPFs: 000.000.000-00\n",
    "    - RGs: 00.000.000-0\n",
    "    - CEPs: 00000-000\n",
    "    \"\"\"\n",
    "    cnpjs = re.findall(r'\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2}', text)\n",
    "    cpfs  = re.findall(r'\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2}', text)\n",
    "    rgs   = re.findall(r'\\d{1,2}\\.\\d{3}\\.\\d{3}-\\d{1}', text)\n",
    "    ceps  = re.findall(r'\\d{5}-\\d{3}', text)\n",
    "    return cnpjs + cpfs + rgs + ceps\n",
    "\n",
    "# ==============================================\n",
    "# 3. Configura√ß√£o para chunking por tokens\n",
    "# ==============================================\n",
    "all_chunks = []          # lista para armazenar todos os chunks\n",
    "global_chunk_number = 0  # contador global de chunks\n",
    "max_tokens = 500         # tamanho m√°ximo do chunk em tokens\n",
    "overlap = 30             # sobreposi√ß√£o de tokens entre chunks\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")  # encoding usado pelo modelo OpenAI\n",
    "\n",
    "def text_to_tokens(text):\n",
    "    \"\"\"Converte texto em tokens\"\"\"\n",
    "    return encoding.encode(text)\n",
    "\n",
    "def tokens_to_text(tokens):\n",
    "    \"\"\"Converte tokens de volta em texto\"\"\"\n",
    "    return encoding.decode(tokens)\n",
    "\n",
    "def chunk_text_by_tokens(text, max_tokens, overlap):\n",
    "    \"\"\"\n",
    "    Divide o texto em chunks de tamanho m√°ximo 'max_tokens' com sobreposi√ß√£o 'overlap'.\n",
    "    - Mant√©m chunks menores que o overlap se contiverem entidades (CPF, CNPJ, etc.).\n",
    "    \"\"\"\n",
    "    tokens = text_to_tokens(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_tokens, len(tokens))\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk_text = tokens_to_text(chunk_tokens)\n",
    "        # Ignora chunks menores que o overlap, exceto se tiver entidades\n",
    "        if len(chunk_tokens) >= overlap or extract_entities_regex(chunk_text):\n",
    "            chunks.append(chunk_text)\n",
    "        start += max_tokens - overlap  # avan√ßa com sobreposi√ß√£o\n",
    "    return chunks\n",
    "\n",
    "# ==============================================\n",
    "# 4. NER com spaCy (Portugu√™s)\n",
    "# ==============================================\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "def extract_spacy_entities(text):\n",
    "    \"\"\"\n",
    "    Extrai somente entidades de Pessoas (PER) e Organiza√ß√µes (ORG) com spaCy.\n",
    "    Remove duplicados.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ in (\"PER\", \"ORG\")]\n",
    "    return list(set(entities))  # remove duplicados\n",
    "\n",
    "# ==============================================\n",
    "# 5. Processamento dos PDFs\n",
    "# ==============================================\n",
    "for pdf_file in os.listdir(pdf_folder):\n",
    "    if not pdf_file.lower().endswith(\".pdf\"):\n",
    "        continue  # ignora arquivos que n√£o s√£o PDF\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "    \n",
    "    full_text = \"\"  # acumula o texto inteiro do documento\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, start=1):\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                page_text = clean_text(page_text)\n",
    "                full_text += \" \" + page_text  # acumula texto do documento\n",
    "                chunks = chunk_text_by_tokens(page_text, max_tokens=max_tokens, overlap=overlap)\n",
    "                \n",
    "                for chunk in chunks:\n",
    "                    # Adiciona chunk original com metadados\n",
    "                    all_chunks.append({\n",
    "                        \"chunk_number\": global_chunk_number,\n",
    "                        \"text\": chunk,\n",
    "                        \"source_file\": pdf_file,\n",
    "                        \"page_number\": i\n",
    "                    })\n",
    "                    global_chunk_number += 1\n",
    "                    \n",
    "                    # Cria chunk apenas com entidades de regex\n",
    "                    entities = extract_entities_regex(chunk)\n",
    "                    if entities:\n",
    "                        entity_text = \" \".join(entities)\n",
    "                        all_chunks.append({\n",
    "                            \"chunk_number\": global_chunk_number,\n",
    "                            \"text\": entity_text,\n",
    "                            \"source_file\": pdf_file,\n",
    "                            \"page_number\": i\n",
    "                        })\n",
    "                        global_chunk_number += 1\n",
    "    \n",
    "    # üîπ Depois de processar todas as p√°ginas, aplica NER ao documento inteiro\n",
    "    spacy_entities = extract_spacy_entities(full_text)\n",
    "    if spacy_entities:\n",
    "        all_chunks.append({\n",
    "            \"chunk_number\": global_chunk_number,\n",
    "            \"text\": \" \".join(spacy_entities),\n",
    "            \"source_file\": pdf_file,\n",
    "            \"page_number\": None   # NER √© do documento inteiro\n",
    "        })\n",
    "        global_chunk_number += 1\n",
    "\n",
    "# ==============================================\n",
    "# 6. Resultados\n",
    "# ==============================================\n",
    "print(f\"Total de chunks gerados: {len(all_chunks)}\")\n",
    "print(\"Exemplo de chunk com metadados:\\n\", all_chunks[0])\n",
    "\n",
    "# ==============================================\n",
    "# 7. Salvar em json\n",
    "# ==============================================\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_file = os.path.join(output_folder, \"chunks.json\")\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"‚úÖ {len(all_chunks)} chunks salvos em {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
